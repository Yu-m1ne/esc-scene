import os
import sys
import shutil
import numpy as np
import tensorflow as tf
import torch
import torch.nn as nn
import torch.nn.functional as F
import subprocess

# å¼•å…¥ä½ çš„æ•°æ®é›†æ¨¡å—
from esc_scene.dataset import create_dataloader

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹æƒé‡è·¯å¾„ (å¿…é¡»æ˜¯ FCN å…¨å·ç§¯ç‰ˆæƒé‡çš„è·¯å¾„)
PTH_MODEL_PATH = "checkpoints/ward_model.pth"

# 2. æ•°æ®é›†è·¯å¾„
DATASET_ROOT = r"F:\Yu_m1ne\dataset\ESC-50-master"

# 3. åœºæ™¯
SCENE = 'ward'

# 4. è¾“å‡ºç›®å½•
OUTPUT_DIR = r"./deploy_models"

# 5. ã€å…³é”®ã€‘ç›´æ¥åœ¨æ­¤å¤„å®šä¹‰ ESP32 æœ€ç»ˆéœ€è¦çš„å°ºå¯¸
# è¿™æ ·å¯¼å‡ºçš„ ONNX åŸç”Ÿå°±æ˜¯è¿™ä¸ªå°ºå¯¸ï¼Œä¸ä¼šæŠ¥é”™
TARGET_HEIGHT = 32
TARGET_WIDTH = 32
INPUT_SHAPE = (1, 1, TARGET_HEIGHT, TARGET_WIDTH)


# ===========================================

# --- å†æ¬¡å£°æ˜ FCN æ¨¡å‹ç»“æ„ (ç¡®ä¿å¯¼å‡ºæ—¶ç”¨çš„æ˜¯å¯¹çš„ç»“æ„) ---
class Esc50NanoSoundDetector(nn.Module):
    def __init__(self, num_classes=4):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 4), stride=(2, 4)),
            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 4), stride=(2, 4)),
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))  # GAP
        )
        self.classifier = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(32, num_classes, kernel_size=1)
        )

    def forward(self, x):
        if x.dim() == 3: x = x.unsqueeze(1)
        x = self.features(x)
        x = self.classifier(x)
        return x


def step1_export_onnx_32x32(onnx_save_path):
    print(f"\n[1/3] Exporting ONNX with shape {INPUT_SHAPE}...")

    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = Esc50NanoSoundDetector(num_classes=4)

    # 2. åŠ è½½æƒé‡
    try:
        # map_location='cpu' é˜²æ­¢æ‰¾ä¸åˆ° GPU æŠ¥é”™
        state_dict = torch.load(PTH_MODEL_PATH, map_location='cpu')
        model.load_state_dict(state_dict)
        print("âœ… Weights loaded successfully.")
    except Exception as e:
        print(f"âš ï¸ WARNING: Failed to load weights ({e}). Exporting with random weights (Architecture check only).")

    model.eval()

    # 3. åˆ›å»º 32x32 çš„ Dummy Input
    # è¿™æ˜¯è§£å†³ "ResizeInputTensorStrict" é”™è¯¯çš„æ ¹æœ¬æ–¹æ³•
    dummy_input = torch.randn(*INPUT_SHAPE)

    # 4. å¯¼å‡º
    torch.onnx.export(
        model,
        dummy_input,
        onnx_save_path,
        opset_version=13,
        input_names=['input'],
        output_names=['output'],
        # ä¸è¦ä½¿ç”¨ dynamic_axesï¼Œè®©å½¢çŠ¶å›ºå®šä¸º 32x32ï¼Œå¯¹ TFLite Micro æœ€å‹å¥½
    )
    print(f"âœ… ONNX saved to: {onnx_save_path}")


def step2_onnx_to_tf(onnx_path, tf_save_path):
    print("\n[2/3] Converting ONNX to TensorFlow...")

    # onnx2tf å‘½ä»¤
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ -ois å‚æ•°ï¼Œå› ä¸º ONNX æœ¬èº«å·²ç»æ˜¯ 32x32 äº†
    cmd = [
        sys.executable, "-m", "onnx2tf",
        "-i", onnx_path,
        "-o", tf_save_path,
        "--non_verbose",
        "-osd"  # ä¾ç„¶éœ€è¦ Output Signature Defs
    ]

    print(f"Running: {' '.join(cmd)}")
    try:
        subprocess.run(cmd, check=True)
        print("âœ… TensorFlow SavedModel generated.")
    except subprocess.CalledProcessError:
        print("âŒ onnx2tf conversion failed.")
        sys.exit(1)


def representative_dataset_gen(dataset_dir, scene, num_samples=100):
    """ç”Ÿæˆ 32x32 çš„æ ¡å‡†æ•°æ®"""
    print(f"Generating calibration data (Resize to {TARGET_HEIGHT}x{TARGET_WIDTH})...")
    dataloader, _ = create_dataloader(dataset_dir, scene, split='val', batch_size=1)

    for i, (label, audio_tensor) in enumerate(dataloader):
        if i >= num_samples: break

        # 1. Resize PyTorch Tensor [1, 1, 80, 501] -> [1, 1, 32, 32]
        # å¿…é¡»ä¸ step1 ä¸­çš„å¯¼å‡ºå°ºå¯¸ä¸€è‡´
        audio_tensor = F.interpolate(
            audio_tensor,
            size=(TARGET_HEIGHT, TARGET_WIDTH),
            mode='bilinear',
            align_corners=False
        )

        # 2. Transpose to NHWC [1, 32, 32, 1]
        # onnx2tf ä¼šè‡ªåŠ¨å¤„ç†æ¨¡å‹ç»´åº¦çš„ NCHW->NHWC è½¬æ¢
        # æ‰€ä»¥æ ¡å‡†æ•°æ®ä¹Ÿå¿…é¡»ç»™ NHWC
        data_nhwc = audio_tensor.permute(0, 2, 3, 1).numpy()

        yield [data_nhwc.astype(np.float32)]


def step3_convert_tflite(tf_path, tflite_path):
    print("\n[3/3] Quantizing to Int8 TFLite...")

    try:
        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)
    except Exception as e:
        print(f"âŒ Failed to load TF model: {e}")
        return

    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = lambda: representative_dataset_gen(DATASET_ROOT, SCENE)

    # å¼ºåˆ¶å…¨æ•´å‹
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8

    try:
        tflite_model = converter.convert()
        with open(tflite_path, 'wb') as f:
            f.write(tflite_model)

        size_kb = len(tflite_model) / 1024
        print(f"\nğŸ‰ SUCCESS! Model saved to: {tflite_path}")
        print(f"ğŸ“Š Final Size: {size_kb:.2f} KB")

    except Exception as e:
        print(f"âŒ Conversion failed: {e}")


if __name__ == "__main__":
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    onnx_path = os.path.join(OUTPUT_DIR, "model_32x32.onnx")
    tf_path = os.path.join(OUTPUT_DIR, "model_tf")
    tflite_path = os.path.join(OUTPUT_DIR, "ward_model_int8.tflite")

    # æ‰§è¡Œå…¨æµç¨‹
    step1_export_onnx_32x32(onnx_path)
    step2_onnx_to_tf(onnx_path, tf_path)
    step3_convert_tflite(tf_path, tflite_path)